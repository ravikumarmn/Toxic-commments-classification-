# Toxic-commments-classification-
![image](https://user-images.githubusercontent.com/82469850/177816792-82ed9f3b-3ea1-4122-826f-adddf2a213cc.png)

# Problem Statement

“To build a multi-headed model that’s capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate.”
Toxic Comment Classifier is a competition that has been organized by Jigsaw/Conversation AI and hosted on Kaggle. The data set for building the classification model was acquired from the competition site and it included the training set as well as the test set. The steps elaborated in the workflow below will describe the entire process from Data Pre-Processing to Model Testing.
